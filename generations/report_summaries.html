<!DOCTYPE html>
<html>
<head>
	<title>Summaries of the doc</title>
    <style>
        .center {
            text-align: center;
        }
    </style>
</head>
<body>
	<h1 class="center">High-Resolution Image Reconstruction with Latent Diffusion Models from Human Brain Activity</h1>
	
		<h2>Page 1</h2>
		<p>This paper presents a new method for reconstructing high-resolution images from human brain activity obtained via functional magnetic resonance imaging (fMRI). The method is based on a latent diffusion model (LDM) termed Stable Diffusion, which reduces the computational cost of diffusion models while preserving their high generative performance. The authors also characterize the inner mechanisms of the LDM by studying how its different components (such as the latent vector of image Z, conditioning inputs C, and different elements of the denoising U-Net) relate to distinct brain functions. They show that their proposed method can reconstruct high-resolution images with high fidelity in a straightforward fashion, without the need for additional training and fine-tuning of complex deep-learning models. Additionally, they provide a quantitative interpretation of different LDM components from a neuroscientific perspective.</p>
	
		<h2>Page 2</h2>
		<p>This study attempts to reconstruct visual images from brain activity using a diffusion model named Stable Diffusion. It is trained on a large dataset and carries high text-to-image generative performance. The results show that the simple framework can reconstruct high-resolution images with high semantic fidelity without any training or fine-tuning of complex deep-learning models. It also provides biological interpretations of each component of the diffusion model, including forward/reverse diffusion processes, U-Net, and latent representations with different noise levels.</p>
	
		<h2>Page 3</h2>
		<p>This page discusses encoding models, which are used by neuroscientists to understand deep-learning models from a biological perspective. The models are built out of features extracted from different components of the deep-learning models, followed by examination of the potential link between model representations and corresponding brain processes. The ability to establish connections between brains and deep-learning models provides us with biological interpretations of the architecture underlying deep-learning models. This approach has been applied primarily to vision science, but it has recently been extended to other sensory modalities and higher functions. The page also explains the methods used in the project, such as the Natural Scenes Dataset, which provides data acquired from a 7-Tesla fMRI scanner, and the latent diffusion model, which consists of an image encoder, image decoder, and text encoder. Finally, it outlines the decoding and encoding analyses used to decode latent representations of the presented image and associated text from fMRI signals, and to build encoding models to predict fMRI signals from different components of the latent diffusion model.</p>
	
		<h2>Page 4</h2>
		<p>This study used latent diffusion models (LDMs) to reconstruct images from fMRI signals in three simple steps: (i) predicting a latent representation of the presented image from fMRI signals within early visual cortex, (ii) processing the latent representation with an decoder of an autoencoder to produce a coarse decoded image, and (iii) decoding latent text representations from fMRI signals within higher (ventral) visual cortex. The accuracy of image reconstruction was evaluated objectively and subjectively. L2-regularized linear regression models were used to construct models from fMRI to the components of LDM. Control analyses were also conducted to generate images using only latent representations or text representations.</p>
	
		<h2>Page 5</h2>
		<p>This page describes a study in which whole-brain voxel-wise encoding models were created to interpret the internal operations of latent variable models (LDMs). Four settings were constructed, each with a different purpose. The first setting built linear models to predict voxel activity from three latent representations of the LDM independently (z, c, and zc). The second setting incorporated z and c into a single model to examine how they differ. The third setting examined how zc changes through the denoising process. The fourth setting extracted features from different layers of U-Net to inspect the last black box associated with LDMs. Model weights were estimated from training data using L2-regularized linear regression, and subsequently applied to test data. Statistical significance was determined by comparing the estimated correlations to the null distribution of correlations between two independent Gaussian random vectors. Results from the study showed that images reconstructed using only z were visually consistent with the original images, but lacked semantic content. Images reconstructed using only c had high semantic fidelity but were visually inconsistent. Images reconstructed using zc could generate high-resolution images with high semantic fidelity. Results from the quantitative evaluation showed that reconstruction quality was stable and accurate across subjects.</p>
	
		<h2>Page 6</h2>
		<p>This page discusses the results of a study on visual reconstruction using Latent Diffusion Models. The study compared the accuracy of reconstructions using objective and subjective criteria, and found that accuracy was highest when both z and c (latent representations of the original image and text annotation) were used. Results showed that z produced the highest accuracy in early visual cortex, c produced the highest accuracy in higher visual cortex, and zc produced a representation similar to z with high accuracy in early visual cortex. The study also found that using zc with a reduced noise level produced a more similar prediction map to the prediction map of z.</p>
	
		<h2>Page 7</h2>
		<p>This page presents results from a study on the prediction performance of a voxel-wise encoding model applied to held-out test images in a single subject. The model was projected onto the inﬂated and ﬂattened cortical surface of the left and right hemispheres, and the brain regions with signiﬁcant accuracy were colored. The study then conducted an additional analysis to compare the unique variance explained by two models, and varied the level of noise added to the latent representation of stimuli from low-level to high-level. Results showed that z predicted voxel activity better than zc across cortex when a small amount of noise was added, and zc predicted voxel activity within higher visual cortex better than z when a higher level of noise was added. Finally, the study examined how the noise-added latent representation changes over the iterative denoising process, and found that the bulk of the semantic content emerged at the middle step of the denoising process.</p>
	
		<h2>Page 8</h2>
		<p>This study examines a novel visual reconstruction method using latent diffusional models (LDMs) to reconstruct high resolution images with high semantic fidelity from human brain activity. Through building encoding models, the authors provide a quantitative interpretation of the internal components of the LDM. Results show that during the early phase of the denoising process, the bottleneck layer of U-Net produces the highest prediction performance across cortex. As denoising progresses, the first layer of U-Net predicts activity within early visual cortex, while the bottleneck layer shifts toward superior predictive power for higher visual cortex. This suggests that image information is compressed within the bottleneck layer at the beginning of the reverse diffusion process, and a functional dissociation among U-Net layers emerges within visual cortex.</p>
	
		<h2>Page 9</h2>
		<p>This page discusses research that has been supported by MEXT/JSPS KAKENHI JP18H05522, JSTCREST JPMJCR18A5 and ERATO JPMJER1801. It references 15 different research papers that explore the relationship between brain activity and artificial intelligence, natural language processing, visual emotion, deep neural networks, and image synthesis. The research covers topics such as self-supervision in natural-image reconstruction from fMRI, decoding the semantic content of natural movies from human brain activity, deep neural networks revealing a gradient in the complexity of neural representations across the ventral stream, and reconstructing complex images from brain activities.</p>
	
		<h2>Page 10</h2>
		<p>This page discusses research on reconstructing natural images from human brain activity using a combination of multiscale local image decoders. It reviews a variety of studies that use deep learning, diffusion probabilistic models, Bayesian reconstructions, and other methods to decode image information from brain activity. It also reviews studies that use language understanding to generate photorealistic images, as well as studies on image super-resolution, unsupervised learning, and generative modeling. The studies discussed were published between 2008 and 2022.</p>
	
		<h2>Page 11</h2>
		<p>This page discusses three papers that explore the connection between score matching and denoising autoencoders (Pascal Vincent, 2011), deep learning for dynamic natural vision (Haiguang Wen et al, 2018), and performance-optimized hierarchical models to predict neural responses in higher visual cortex (Daniel LK Yamins et al, 2014). The page is licensed under the CC-BY 4.0 International license and the copyright holder is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.</p>
	
</body>
</html>