<!DOCTYPE html>
<html>
<head>
	<title>Summaries of the doc</title>
    <style>
        .center {
            text-align: center;
        }
    </style>
</head>
<body>
	<h1 class="center">High-Resolution Image Reconstruction with Latent Diffusion Models from Human Brain Activity</h1>
	
		<h2>Page 1</h2>
		<p>This paper presents a new method based on a latent diffusion model (LDM) termed Stable Diffusion to reconstruct high-resolution images with high semantic fidelity from human brain activity obtained via functional magnetic resonance imaging (fMRI). It explores the inner mechanisms of the LDM by studying how its different components (latent vector of image Z, conditioning inputs C, and different elements of the denoising U-Net) relate to distinct brain functions. The proposed method does not require any additional training or fine-tuning of complex deep-learning models. It also provides a quantitative interpretation of different LDM components from a neuroscientific perspective.</p>
	
		<h2>Page 2</h2>
		<p>This page discusses the reconstruction of visual images from brain activity using deep-learning models and algorithms, such as generative adversarial networks (GANs) and self-supervised learning. It introduces the use of diffusion models (DMs) and latent diffusion models (LDMs) which are deep genera-tive models and have been gaining attention in recent years. It introduces a new LDM named Stable Diffusion and shows how it can reconstruct high-resolution images with high semantic fidelity without any training or fine-tuning of complex deep-learning models. It also provides biological interpretations of each component of the LDM.</p>
	
		<h2>Page 3</h2>
		<p>This page discusses encoding models, which are predictive models of brain activity built out of features extracted from different components of deep-learning models. It then examines the potential link between model representations and corresponding brain processes, and the ability to establish connections between brains and deep-learning models. It then introduces the Natural Scenes Dataset, which provides data acquired from a 7-Tesla fMRI scanner over 30â€“40 sessions during which each subject viewed three repetitions of 10,000 images. It then presents an overview of its methods, which includes a latent diffusion model, decoding analysis, and encoding analysis.</p>
	
		<h2>Page 4</h2>
		<p>This study used latent diffusion models (LDMs) to reconstruct images from fMRI signals within early and higher (ventral) visual regions. A U-Net architecture was used to generate latent representations from text inputs, and an autoencoder was used to compress the input. The model was trained on a large dataset, and was used to generate and modify images based on text input. The accuracy of image reconstruction was evaluated using perceptual similarity metrics (PSMs) and human raters. The model was tested by conducting two-way identification experiments to examine whether the image reconstructed from fMRI was more similar to the corresponding original image than a randomly picked reconstructed image.</p>
	
		<h2>Page 5</h2>
		<p>This page describes a study that attempted to interpret the internal operations of Latent Dirichlet Allocation (LDM) models by mapping them to brain activity. To do this, four different settings were constructed, each of which involved building linear models to predict voxel activity from the latent representations of the LDM (z, c, and zc). The analysis enabled a quantitative interpretation of the image-to-image process and insight into the internal dynamics of the denoising process. Finally, features were extracted from different layers of U-Net, and the layer with the highest accuracy for each voxel was identified for each step. The results showed that images reconstructed using zc were able to generate high-resolution images with high semantic fidelity. The evaluation of the results was done using Pearson's correlation coefficients, and the statistical threshold was set at P< 0.05.</p>
	
		<h2>Page 6</h2>
		<p>This page summarizes the results of a study that aimed to understand how low-level visual appearance and high-level semantic content of original stimuli can be captured by a method that uses two linear regression models from fMRI activity to latent representations of a latent-directed model (LDM). The objective evaluation showed that images reconstructed using zcare generally associated with higher accuracy values across different metrics than images reconstructed using only zorc. When only zwas used, accuracy values were particularly high for PSMs derived from early layers of CLIP and CNN. On the other hand, when only cwas used, accuracy values were higher for PSMs derived from late layers. In the subjective evaluation, accuracy values of images obtained from care higher than those obtained from z, while zcresulted in the highest accuracy compared with the other two methods. The encoding models for the three types of latent representations associated with the LDM showed high prediction performance at the back of the brain, visual cortex, with zproducing high prediction performance in the posterior part of visual cortex, namely early visual cortex, and cproducing the highest prediction performance in higher visual cortex.</p>
	
		<h2>Page 7</h2>
		<p>This page presents the results of a study conducted to compare the prediction performance of a voxel-wise encoding model applied to held-out test images in a single subject. The results showed that when a small amount of noise was added, the original images predicted voxel activity better than the noise-added latent representation across cortex. When the level of noise was increased, the latent representation predicted voxel activity within higher visual cortex better than the original images, indicating that the semantic content of the image was gradually emphasized. The results also showed that during the early stages of the denoising process, the original images signals dominated prediction of fMRI signals, while during the middle step of the denoising process, the noise-added latent representation predicted activity within higher visual cortex much better than the original images, indicating that the bulk of the semantic content emerges at this stage. The copyright holder for this preprint is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.</p>
	
		<h2>Page 8</h2>
		<p>This study proposes a novel visual reconstruction method using latent diffusion models (LDMs) to reconstruct high-resolution images with high semantic fidelity from human brain activity. It is shown that the method does not require training or fine-tuning of complex deep-learning models, but only requires simple linear mappings from fMRI to latent representations within LDMs. The internal components of the LDM are quantitatively interpreted by building encoding models. Results suggest that semantic content emerges throughout the inverse diffusion process, and layer-wise characterization of U-Net is performed. A quantitative interpretation of image-to-image transformations with different noise levels is also provided.</p>
	
		<h2>Page 9</h2>
		<p>This page discusses research conducted by JSPS KAKENHI (19H05725) and MEXT/JSPS KAKENHI JP18H05522, JSTCREST JPMJCR18A5, and ERATO JPMJER1801. It references fourteen sources that discuss the use of deep neural networks to bridge cognitive neuroscience and artificial intelligence, self-supervision in natural-image reconstruction from fMRI, the contributions of functional and deep neural network features to representational similarity of scenes in the human brain, diffusion models beating GANs on image synthesis, the neural representation of visually evoked emotion, generic decoding of seen and imagined objects, neural decoding of visual imagery during sleep, decoding the semantic content of natural movies from human brain activity, JPEG artifact correction using denoising diffusion restoration models, identifying natural images from human brain activity, task-optimized neural networks replicating human auditory behavior, recurrence being required to capture the representational dynamics of the human visual system, distinct dimensions of emotion in the human brain, cascaded tuning to amplitude modulation for natural sound recognition, Imagenet classification with deep convolutional neural networks, feature-space selection with banded ridge regression, and fully convolutional naturalistic video reconstruction from brain activity.</p>
	
		<h2>Page 10</h2>
		<p>This page presents a collection of research papers that explore the reconstruction of natural images from human brain activity using a combination of multiscale local image decoders. The papers discuss various models such as Bigbigan, Voxel-wise Encoding Model, Bayesian Reconstruction, Distributed Representations of Words, Iterative Refinement, Denoising Diffusion Probabilistic Models, Language Supervision, Image Super-Resolution, Unpaired Image Translation, Event-Based Time Scales, Predictive Processing, Generative Adversarial Networks, End-to-End Deep Image Reconstruction, Stochastic Differential Equations, Rethinking the Inception Architecture, and Whole-Brain Functional Connectivity. These models are used to reconstruct natural images, speech, and language from brain activity.</p>
	
		<h2>Page 11</h2>
		<p>This page is a reference list for three research papers. The first paper by Pascal Vincent is about the connection between score matching and denoising autoencoders. The second paper by Haiguang Wen et al. is about neural encoding and decoding with deep learning for dynamic natural vision. The third paper by Daniel LK Yamins et al. is about performance-optimized hierarchical models predicting neural responses in higher visual cortex. All three papers are licensed under a CC-BY 4.0 International license.</p>
	
</body>
</html>